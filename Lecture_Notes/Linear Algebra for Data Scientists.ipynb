{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Numbers \n",
    "\n",
    "We will need to use some basic facts about complex numbers, so let's just collect them here.  \n",
    "\n",
    "* By $z\\in\\mathbb{C}$, we mean $z = z_{r} + i z_{i}$, where $i^{2}=-1$.\n",
    "* For $z\\in\\mathbb{C}$, we define its _conjugate_ $z^{\\ast} = z_{r}-iz_{i}$\n",
    "* For $z\\in\\mathbb{C}$, we define its _modulus_ $|z| = \\left(z^{\\ast}z\\right)^{1/2}$\n",
    "\n",
    "_Problem_: Show that $|z|^{2} = z_{r}^{2} + z_{i}^{2}$.  \n",
    "\n",
    "* (de Moivre representation) For $z\\in \\mathbb{C}$, we can write $z=|z|e^{i\\theta}$ where $\\tan(\\theta)=\\frac{z_{i}}{z_{r}}$.\n",
    "\n",
    "_Problems_: Given that $e^{i\\theta}=\\cos(\\theta) + i\\sin(\\theta)$, show\n",
    "* $(e^{i\\theta})^{\\ast} = e^{-i\\theta}$\n",
    "* $\\cos(\\theta) = \\frac{1}{2}\\left(e^{i\\theta} + e^{-i\\theta} \\right)$\n",
    "* $\\sin(\\theta) = \\frac{1}{2i}\\left(e^{i\\theta} - e^{-i\\theta} \\right)$\n",
    "* $\\cos\\left(\\theta + \\phi\\right) = \\cos\\left(\\theta\\right)\\cos\\left(\\phi\\right) - \\sin\\left(\\theta\\right)\\sin\\left(\\phi\\right)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra Theory \n",
    "\n",
    "We have some definitions to keep in mind about different types of matrices.  \n",
    "\n",
    "* If $m=n$, we say the matrix $A$ is square.  \n",
    "* If $m<n$, we say the matrix problem $A{\\bf x}={\\bf b}$ is _underdetermined_, i.e. there are fewer equations than there are unknowns.  \n",
    "* If $m>n$, we say the matrix problem $A{\\bf x}={\\bf b}$ is _overdetermined_, i.e. there are more equations than there are unknowns.  \n",
    "* For a $m\\times n$ matrix $A$, we define its _transpose_, $A^{T}$ to be the $n \\times m$ matrix with entries $A^{T}_{ij} = A_{ji}$.  Visually, we see that $A^{T}$ is flipped relative to $A$, i.e. \n",
    "$$\n",
    "A^{T} = \\begin{pmatrix} A_{11} & A_{21} & \\cdots & A_{m1} \\\\ A_{12} & A_{22} & \\cdots & A_{m2}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ A_{1n} & A_{2n} & \\cdots & A_{mn} \\end{pmatrix},~ A^{T}:\\mathbb{C}^{m}\\rightarrow \\mathbb{C}^{n}\n",
    "$$\n",
    "* For a $m\\times n$ matrix $A$, we define its _Hermitian conjugate_, $A^{\\dagger}$ to be the $n \\times m$ matrix $(A^{\\ast})^{T}$.  In other words, just conjugate all entries and then take the transpose.  \n",
    "\n",
    "* Using the rules of matrix multiplication, if $A$ is an $m\\times n$ matrix, then $A^{\\dagger}A$ is an $n \\times n$ matrix and $AA^{\\dagger}$ is a $m \\times m$ matrix.  Thus for an overdetermined problem, we can transform it into a square problem of the form \n",
    "$$\n",
    "A^{\\dagger}A {\\bf x} = A^{\\dagger}{\\bf b}.\n",
    "$$\n",
    "* For $m\\times n$ matrix $A$, the _kernel_ of $A$, or $\\text{ker}(A)$ is defined so that \n",
    "$$\n",
    "\\text{ker}(A) = \\left\\{{\\bf x}\\in \\mathbb{C}^{n} : A{\\bf x} = 0 \\right\\}.\n",
    "$$\n",
    "* For $m\\times n$ matrix $A$, the _range_ of $A$, or $\\text{rng}(A)$ is defined so that \n",
    "$$\n",
    "\\text{rng}(A) = \\left\\{{\\bf y}\\in \\mathbb{C}^{m} : \\exists {\\bf x}\\in \\mathbb{C}^{n}, A{\\bf x} = {\\bf y} \\right\\}.\n",
    "$$\n",
    "\n",
    "### Inner Products, Orthogonality, and Projections\n",
    "* Note, for ${\\bf x},{\\bf y} \\in \\mathbb{C}^{n}$, we define the _inner-product_  $\\left<{\\bf x},{\\bf y}\\right>$ to be\n",
    "$$\n",
    "\\left<{\\bf x},{\\bf y}\\right> = {\\bf y}^{\\dagger}{\\bf x} = \\sum_{j=1}^{n}y^{\\ast}_{j}x_{j}.\n",
    "$$\n",
    "* We say two vectors ${\\bf x},{\\bf y} \\in \\mathbb{C}^{n}$ are _orthogonal_, or ${\\bf x}\\perp {\\bf y}$, if and only if \n",
    "$\\left<{\\bf x},{\\bf y}\\right> = 0.$\n",
    "* \n",
    "\n",
    "_Problems_:\n",
    "* Show $\\left<{\\bf x},{\\bf y}\\right> = \\left(\\left<{\\bf y},{\\bf x}\\right>\\right)^{\\ast}$\n",
    "* For a square matrix $A$, show $\\left<A{\\bf x},{\\bf y}\\right> = \\left<{\\bf x},A^{\\dagger}{\\bf y}\\right>$.     \n",
    "\n",
    "\n",
    "\n",
    "### Solvability for Matrices\n",
    "* The central question here is, can we solve $A{\\bf x} = {\\bf b}$ where we treat $A$ and ${\\bf b}$ as given?\n",
    "* We now suppose that $A$ is a $m\\times n$ matrix of complex values so that: $A: \\mathbb{C}^{n} \\rightarrow \\mathbb{C}^{m}\n",
    "$\n",
    " #### Fundamental Theorem of Linear Algebra\n",
    "> For $m\\times n$ matrix $A$, so that $A: \\mathbb{C}^{n} \\rightarrow \\mathbb{C}^{m}\n",
    "$, we have \n",
    "\\begin{align}\n",
    "\\mathbb{C}^{n} & = \\text{ker}(A) \\oplus \\text{rng}(A^{\\dagger}),\\\\ \n",
    "\\mathbb{C}^{m} & = \\text{ker}(A^{\\dagger}) \\oplus \\text{rng}(A),\n",
    "\\end{align}\n",
    "with the further constraint that \n",
    "$$\n",
    "\\text{dim}\\left(\\text{rng}(A)\\right) = \\text{dim}\\left(\\text{rng}(A^{\\dagger})\\right)\n",
    "$$\n",
    "Note, the $\\oplus$ implies the subspaces are orthogonal to one another.  \n",
    "\n",
    "In plain language, the Fundamental Theorem means for example that for ${\\bf x} \\in \\mathbb{C}^{n}$, ${\\bf x} = {\\bf k} + {\\bf r}$, ${\\bf k}\\in \\text{ker}(A)$, ${\\bf r}\\in \\text{rng}(A^{\\dagger})$,  $\\left<{\\bf k},{\\bf r}\\right>=0$.  An equivalent statement holds true in the target space $\\mathbb{C}^{m}$.\n",
    "\n",
    "The Fundamental Theorem is complemented by the definition of the _rank_ of a matrix $A$.  \n",
    "\n",
    "#### Rank of A\n",
    "> For $m\\times n$ matrix $A$, $\\text{rank}(A)$ is defined to be the dimension of the column space of $A$.  Through computational row-reduction methods and using the Fundamental Theorem above, we can prove that  \n",
    "$$\n",
    "\\text{rank}(A) = \\text{dim}\\left(\\text{rng}(A)\\right) = \\text{dim}\\left(\\text{rng}(A^{\\dagger})\\right) = \\text{rank}(A^{\\dagger}).\n",
    "$$\n",
    "\n",
    "In words, this says that the dimension of the column and row space of a matrix $A$ must be the same.  A basic question then in linear algebra is how to determine and ultimately encode the rank of a matrix $A$ since this quantifies the size of the subspace that the matrix $A$ can encode by way of transformation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This theorem gives us two possible outcomes: \n",
    "* There exists ${\\bf y}$ such that $A^{\\dagger}{\\bf y}=0$ but $\\left<{\\bf b},{\\bf y} \\right>\\neq 0$.  Then **no** solution ${\\bf x}$ exists to the problem.  \n",
    "* $\\left<{\\bf b},{\\bf y} \\right> = 0$ for all ${\\bf y}\\neq 0$ and $A^{\\dagger}{\\bf y} = 0$.  Then at least one solution exists.\n",
    "\n",
    "In the second case, we have to also check if $A{\\bf x}=0$ has nontrivial solutions.  \n",
    "* If it does, then the $A{\\bf x}={\\bf b}$ problem has solutions of the form  \n",
    "$$\n",
    "{\\bf x} = {\\bf x}_{h} + {\\bf x}_{p}, ~ {\\bf x}_{h} \\in \\text{ker}(A),\n",
    "$$ \n",
    "and $A{\\bf x}_{p}={\\bf b}$.  Note, we have an infinite number of solutions since for any constant $c\\in \\mathbb{C}$, we have \n",
    "$$\n",
    "A\\left(c{\\bf x}_{h} + {\\bf x}_{p} \\right) = c A{\\bf x}_{h} + A{\\bf x}_{p} = {\\bf b}.\n",
    "$$\n",
    "* If the only solution to $A{\\bf y}=0$ is ${\\bf y}=0$ (i.e. $\\text{ker}(A)=\\left\\{{\\bf 0}\\right\\}$), then there is a unique solution to $A{\\bf x}={\\bf b}$.\n",
    "* If $m=n$ and $\\text{ker}(A)=\\left\\{{\\bf 0}\\right\\}$, then we say a matrix $A^{-1}$ exists, where $AA^{-1}=A^{-1}A=I$ and we have the **unique** solution \n",
    "$$\n",
    "{\\bf x} = A^{-1}{\\bf b}.\n",
    "$$\n",
    "* There are several equivalent criteria which establish whether a square matrix $A$ has an inverse.  The most important are \n",
    "    * The only solution to $A{\\bf x}=0$ is ${\\bf x}=0$.\n",
    "    * The determinant of $A$ is non-zero, or $\\text{det}(A)\\neq 0$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms\n",
    "\n",
    "We also need a notion of the size of a vector.  We thus define the _2-norm_ of a vector ${\\bf x}\\in \\mathbb{C}^{n}$ to be $\\left|\\left|{\\bf x}\\right|\\right|_{2}$ where\n",
    "\n",
    "$$\n",
    "\\left|\\left|{\\bf x}\\right|\\right|_{2} = \\left(\\sum_{j=1}^{n}\\left|x_{j} \\right|^{2} \\right)^{1/2}.\n",
    "$$\n",
    "\n",
    "_Problems_: Show \n",
    "* $\\left|\\left|{\\bf x}\\right|\\right|_{2} = 0 $ if and only if ${\\bf x} = {\\bf 0}$.\n",
    "* For any scalar values $\\lambda \\in \\mathbb{C}$, $\\left|\\left|\\lambda{\\bf x}\\right|\\right|_{2} = \\left|\\lambda\\right| \\left|\\left|{\\bf x}\\right|\\right|_{2}$.\n",
    "* If we define $\\hat{{\\bf x}} = {\\bf x}/\\left|\\left|{\\bf x}\\right|\\right|_{2}$, then $\\left|\\left|\\hat{{\\bf x}}\\right|\\right|_{2} = 1 $.\n",
    "\n",
    "Underdetermined problems are a bit messier to cope with.  To do so, we need to get more comfortable with norms.  To that end then:\n",
    "\n",
    "_Problems_: Show \n",
    "* $\\left|\\left|{\\bf x}\\right|\\right|^{2}_{2} = {\\bf x}^{\\dagger}{\\bf x} = \\left<{\\bf x}, {\\bf x}\\right>$\n",
    "* Show $(A^{\\dagger}A)^{\\dagger} = A^{\\dagger}A$ and $(AA^{\\dagger})^{\\dagger} = AA^{\\dagger}$.  Matrices which are equal to their own transpose are called _symmetric_ or _self-adjoint_.\n",
    "* Suppose $A^{\\dagger}{\\bf y} = 0$.  Show that if a solution to $A{\\bf x}={\\bf b}$ exists, then $\\left<{\\bf b},{\\bf y}\\right>=0$.  \n",
    "* For ${\\bf x},{\\bf y} \\in \\mathbb{C}^{n}$, show the _Cauchy-Schwarz_ inequality \n",
    "$$\n",
    "\\left|\\left<{\\bf x},{\\bf y}\\right>\\right| = \\left|\\sum_{j=1}^{n}x_{j}y_{j}\\right| \\leq \\left(\\sum_{j=1}^{n}|x_{j}|^{2}\\right)^{1/2}\\left(\\sum_{j=1}^{n}|y_{j}|^{2}\\right)^{1/2} = \\left|\\left|{\\bf x}\\right|\\right|_{2}\\left|\\left|{\\bf y}\\right|\\right|_{2}.\n",
    "$$\n",
    "Note, the triangle inequality for real numbers, $|a+b|\\leq |a|+|b|$ gives us via an inductive argument that \n",
    "$$\n",
    "\\left|\\sum_{j=1}^{n}x_{j}y_{j}\\right| \\leq \\sum_{j=1}^{n}\\left|x_{j}\\right|\\left|y_{j}\\right|\n",
    "$$\n",
    "* Using the Cauchy-Schwarz inequality and the triangle inequality, where $|a+b|\\leq |a|+|b|$, show \n",
    "$$\n",
    "\\left|\\left|{\\bf x}+{\\bf y}\\right|\\right|_{2} \\leq \\left|\\left|{\\bf x}\\right|\\right|_{2} + \\left|\\left|{\\bf y}\\right|\\right|_{2}.\n",
    "$$\n",
    "\n",
    "### Norms of Matrices\n",
    "We can also directly measure the size of matrices by way of _matrix norms_.  Given $m\\times n$ matrix $A$, a very popular choice of matrix norm is the _Frobenius norm_, say $\\left|\\left|A\\right|\\right|_{F}$, where\n",
    "$$\n",
    "\\left|\\left|A\\right|\\right|_{F} = \\left(\\text{tr}\\left(AA^{\\dagger}\\right) \\right)^{1/2} = \\left(\\text{tr}\\left(A^{\\dagger}A\\right) \\right)^{1/2}\n",
    "$$\n",
    "Note, for a square $n\\times n$ matrix $B$, the _trace_ of $B$, $\\text{tr}(B)$ is given by\n",
    "$$\n",
    "\\text{tr}(B) = \\sum_{j=1}^{n} b_{jj} \n",
    "$$\n",
    "In other words, the trace is the sum of the diagonal elements.  \n",
    "\n",
    "_Problem_: Show that \n",
    "$$\n",
    "\\left|\\left|A\\right|\\right|_{F} = \\left(\\sum_{j=1}^{m}\\sum_{l=1}^{n}\\left|a_{jl}\\right|^{2}\\right)^{1/2}\n",
    "$$\n",
    "## Eigenvalues and Symmetric Matrices\n",
    "\n",
    "We say that a constant $\\lambda \\in \\mathbb{C}$ is an _eigenvalue_ of a square matrix $A$ if there exists some vector ${\\bf v}$ such that \n",
    "$$\n",
    "A{\\bf v} = \\lambda {\\bf v}.\n",
    "$$\n",
    "We call ${\\bf v}$ an _eigenvector_.  Note, the above equation is equivalent to \n",
    "$$\n",
    "(A - \\lambda I){\\bf v} = 0,\n",
    "$$\n",
    "which is equivalent to requiring that $\\text{det}(A-\\lambda I)=0$.  Note, we are usually obliged to worry about complex eigenvalues and eigenvectors for even real matrices.  \n",
    "\n",
    "_Problem_: For \n",
    "$$\n",
    "A = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0\\end{pmatrix},\n",
    "$$\n",
    "find its eigenvalues and eigenvectors.  \n",
    "\n",
    "However, we are only going to work with symmetric matrices where $A^{T}=A$.  In this case, one can show that $\\lambda$ and its corresponding ${\\bf v}$ are real.  This essentially comes from the formula\n",
    "$$\n",
    "\\lambda \\left|\\left|{\\bf v}\\right|\\right|^{2} = \\left<A{\\bf v},{\\bf v} \\right> = \\left<{\\bf v},A{\\bf v} \\right>.\n",
    "$$\n",
    "Thus, we get for symmetric matrices the formula for the eigenvalues $\\lambda$\n",
    "$$\n",
    "\\lambda = \\left<A\\hat{{\\bf v}},\\hat{{\\bf v}} \\right>, ~ \\hat{{\\bf v}} = {\\bf v}/ \\left|\\left|{\\bf v}\\right|\\right|.\n",
    "$$\n",
    "\n",
    "_Problem_: Show \n",
    "* For symmetric matrix $A$ that if it has two unequal eigenvalues, say $\\lambda_{1}$ and $\\lambda_{2}$, with \n",
    "$$\n",
    "A{\\bf v}_{1} = \\lambda_{1}{\\bf v}_{1}, ~ A{\\bf v}_{2} = \\lambda_{2}{\\bf v}_{2}\n",
    "$$\n",
    "that $\\left<{\\bf v}_{1},{\\bf v}_{2} \\right>=0$.\n",
    "* Suppose that for a symmetric matrix $A$, we find two eigenvectors, say ${\\bf v}_{1}$ and ${\\bf v}_{2}$ for the same eigenvalue $\\lambda$.  Show that one can find two eigenvectors $\\hat{\\bf v}_{1}$ and $\\hat{\\bf v}_{2}$ such that \n",
    "$$\n",
    "A\\hat{\\bf v}_{j} = \\lambda \\hat{\\bf v}_{j}, ~ \\left<\\hat{{\\bf v}}_{1}, \\hat{{\\bf v}}_{2}\\right>=0.\n",
    "$$\n",
    "* Suppose one has found $j-1$ eigenvalues and eigenvectors, say $\\lambda_{l}$ and $\\hat{{\\bf v}}_{l}$, $l=1,\\cdots,j-1$, of the matrix $A$, and $\\left<\\hat{{\\bf v}}_{l},\\hat{{\\bf v}}_{k} \\right>=\\delta_{lk}$.  Note, we do not require that the values $\\lambda_{l}$ all be distinct, i.e. eigenvalues can repeat.  Suppose one has a unit vector $\\hat{{\\bf x}}$ such that \n",
    "$$\n",
    "\\left<\\hat{{\\bf x}},\\hat{{\\bf v}}_{l} \\right> = 0.\n",
    "$$\n",
    "Show that \n",
    "$$\n",
    "\\left<\\hat{{\\bf x}},\\sum_{l=1}^{j-1}c_{l}\\hat{{\\bf v}}_{l} \\right>  = 0, ~ c_{l} \\in \\mathbb{R}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
