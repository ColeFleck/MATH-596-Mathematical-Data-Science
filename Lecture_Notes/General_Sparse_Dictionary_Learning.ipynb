{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3321247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_sample_image\n",
    "from sklearn.feature_extraction import image\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b805af64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pooch\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/ccurtis/.conda/envs/math_596_fall_2024/lib/python3.12/site-packages (from pooch) (4.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ccurtis/.conda/envs/math_596_fall_2024/lib/python3.12/site-packages (from pooch) (24.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ccurtis/.conda/envs/math_596_fall_2024/lib/python3.12/site-packages (from pooch) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ccurtis/.conda/envs/math_596_fall_2024/lib/python3.12/site-packages (from requests>=2.19.0->pooch) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ccurtis/.conda/envs/math_596_fall_2024/lib/python3.12/site-packages (from requests>=2.19.0->pooch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ccurtis/.conda/envs/math_596_fall_2024/lib/python3.12/site-packages (from requests>=2.19.0->pooch) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ccurtis/.conda/envs/math_596_fall_2024/lib/python3.12/site-packages (from requests>=2.19.0->pooch) (2024.8.30)\n",
      "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: pooch\n",
      "Successfully installed pooch-1.8.2\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install pooch # You might need to run this once  \n",
    "                             # and then you can delete it..  \n",
    "                             # Needed for Problem 3.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5f8660",
   "metadata": {},
   "source": [
    "## General Sparse Dictionary Learning Problem\n",
    "\n",
    "So the basic concept we're working with here, summarizing the last couple weeks of lecture is:   \n",
    "\n",
    "> **Dictionary Learning**: Given data $n\\times m$ real-value data matrix $Y$, we want to find a $n\\times p$ _dictionary_ $A_{\\ast}$ and sparse encoding of $Y$, say $p\\times m$ $Z_{\\ast}$ which solves\n",
    "$$\n",
    "A_{\\ast}, Z_{\\ast} = \\text{arg min}_{A, Z} \\left|\\left|Y-AZ\\right|\\right|_{F} + \\lambda \\left|\\left|Z\\right|\\right|_{1}, ~ \\lambda > 0.\n",
    "$$\n",
    "Each column of $A$ is called an _atom_.  We seek _overcomplete_ dictionaries so that $n<p$, and to make the problem more tractable, we should have even more data than we have atoms so that $n<p<m$.  \n",
    "\n",
    "> **Greedy Algorithms**: To solve \n",
    "$$\n",
    "\\text{min}_{A, Z} \\left|\\left|Y-AZ\\right|\\right|_{F} + \\lambda \\left|\\left|Z\\right|\\right|_{1}\n",
    "$$\n",
    "we make an initial guess for $A_{0}$ and then solve in two stages:\n",
    "\\begin{align*}\n",
    "\\text{Find} ~ Z_{1} = & \\text{arg min}_{Z} \\left|\\left|Y-A_{0}Z\\right|\\right|_{F} + \\lambda \\left|\\left|Z\\right|\\right|_{1} \\\\\n",
    "\\text{Find} ~ A_{1} = & \\text{arg min}_{A} \\left|\\left|Y-AZ_{1}\\right|\\right|_{F}.      \n",
    "\\end{align*}\n",
    "Repeat then until you (hopefully) get convergence.  Note, by saying \"greedy\" we mean that the method will almost certainly converge, and thankfully since this is a convex optimization problem, we are guaranteed that any local minima is a global one.  \n",
    "\n",
    "\n",
    "> **Computing $D_{A}\\left|\\left|Y-AZ\\right|\\right|_{F}^{2}$**: So if we define the function $f(A)=\\left|\\left|Y-AZ\\right|\\right|_{F}^{2}$, then we can find its directional derivative at $A$ with respect to the direction $W$, denoted as $<D_{A}f(A), W>=\\text{tr}((D_{A}f(A))^{T}W)$, via the formula:\n",
    "$$\n",
    "<D_{A}f(A), W> = \\lim_{\\epsilon\\rightarrow 0}\\frac{f(A+\\epsilon W) - f(A)}{\\epsilon} = \\left<-2Z(Y-AZ)^{T}, W\\right>\n",
    "$$\n",
    "Thus, if we want to find $A_{\\ast}$ such that $D_{A}f(A)|_{A=A_{\\ast}}=0$, then we see that $A_{\\ast}$ solves\n",
    "$$\n",
    "Z(Y-A_{\\ast}Z)^{T} = 0 \\implies A_{\\ast}ZZ^{T} = YZ^{T}.\n",
    "$$\n",
    "\n",
    "> **Moore-Penrose Pseudoinverse**: If for the $p\\times m$ real-valued matrix $Z$ we have $\\text{rank}(Z)=p$ (remember $p<m$), then we can find the rank-reduced SVD of $Z$ which is $Z = U\\Sigma_{p}V_{p}^{T}$ where $\\Sigma_{p}$ is $p\\times p$ and has strictly positive diagonal entries so $\\Sigma_{p}^{-1}$ exists.  Our full rank condition ensures that $U^{T}U=UU^{T}=I$, but we only have $V_{p}^{T}V_{p}=I$ with $V_{p}V_{p}^{T}$ in general being a projection matrix, not the identity. We then see that \n",
    "$$\n",
    "ZZ^{T} = U\\Sigma_{p}^{2}U_{p}^{T}\n",
    "$$\n",
    "is of full rank and thus invertible.  Therefor the problem\n",
    "$$\n",
    "A_{\\ast}ZZ^{T} = YZ^{T}\n",
    "$$\n",
    "can now be written as \n",
    "$$\n",
    "A_{\\ast} = YZ^{-P} = YV_{p}\\Sigma_{p}^{-1}U^{T}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429b85d",
   "metadata": {},
   "source": [
    "### Example: The Noisy Racoon\n",
    "\n",
    "So it shouldn't be surprising that dictionary learning is something that you can do via library calls in Scikit-Learn.  To wit, there is a full tutorial that walks you through the process of using dictionary learning to do image denoising at:\n",
    "\n",
    "[Noisy Raccoon](https://scikit-learn.org/stable/auto_examples/decomposition/plot_image_denoising.html#sphx-glr-auto-examples-decomposition-plot-image-denoising-py)\n",
    "\n",
    "As you can readily see, you can (and should) download the Jupyter notebook that implements the process soup-to-nuts.  So me making you recreate all of it is... silly.  \n",
    "\n",
    "That said, I can ask you questions that hopefully make you crawl through the code line-by line, and of course there is always theory to fuss over, so let's get to it.  Note, please make all changes to code in that notebook, but please report results in this one.  Upload both to your homework folder.  \n",
    "\n",
    "Anyway, in the code, dictionary learning is done via the function `MiniBatchDictionaryLearning()` which solves:\n",
    "\n",
    "$$\n",
    "\\min_{A, Z} \\left|\\left|Y - AZ\\right|\\right|_{F} + \\lambda \\left|\\left|Z\\right|\\right|_{1, 1}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\left|\\left|Z\\right|\\right|_{1, 1} = \\sum_{j=1}^{p}\\sum_{l=1}^{m}|z_{jl}|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8370d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math_596_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
