{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb4bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fft import fft, ifft, fftshift\n",
    "from sklearn.linear_model import Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from copy import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf0dc46-41dd-44ce-9b33-67063fc223b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_set_1.pkl', 'rb') as f:\n",
    "    data_set_1 = pickle.load(f)\n",
    "\n",
    "xvals1 = data_set_1[:, 0]\n",
    "yvals1 = data_set_1[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b819075-29a7-4bec-9cee-fe1d5150fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xp_mat_maker(xvals, p):\n",
    "    xp = np.ones((xvals.size, p+1))\n",
    "    for jj in range(p):\n",
    "        xp[:, jj+1] = xvals * xp[:, jj]\n",
    "    return xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3417f29-366c-4ed4-abea-0eb4a5fcb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_solve(mat, yvals):\n",
    "    u, s, vt = np.linalg.svd(mat, full_matrices=False)\n",
    "    alpha = (vt.T @ np.diag(1./s) @ u.T) @ yvals.reshape(-1, 1)\n",
    "    error = np.linalg.norm(mat @ alpha - yvals.reshape(-1, 1))\n",
    "    return alpha, error "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5779d8a3-bdf0-48be-8bfe-0bf6cf77ac1f",
   "metadata": {},
   "source": [
    "**Problem 1**: To get a feel for how LASSO regression works, we will study the problem of solving \n",
    "$$\n",
    "A{\\bf x} = {\\bf b}, ~ A \\in \\mathbb{R}^{n\\times m}, {\\bf b} \\in \\mathbb{R}^{n}\n",
    "$$\n",
    "via least squares with and without LASSO.  Thus we study minimizing the objective function $f({\\bf x})$ where \n",
    "$$\n",
    "f({\\bf x}) = \\left|\\left|{\\bf b} - A{\\bf x}\\right|\\right|^{2}_{2} + \\alpha \\left|\\left|{\\bf x}\\right|\\right|_{1}, ~ \\alpha \\geq 0.\n",
    "$$\n",
    "\n",
    "We choose $A$ and ${\\bf b}$ at random from Gaussian distributions of zero mean and variance $\\sigma$.  By varying $n$, $m$, and $\\sigma$, generate distributions of the values of ${\\bf x}$ for $\\alpha=.1$ and $\\alpha=1$.  Characterize the impact of LASSO on the spread in the values of ${\\bf x}$.  Also, generate affiliated distributions of the error in your fits.  Describe any differences you see between LASSO fits and fits done with classic least-squares.  Refer to Sections 3.3 and 4.3 of the textbook for guidance and further details.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa74660-4712-4722-9828-bddaddabb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmodel = # choices choices\n",
    "nmeasure = # choices choices\n",
    "varsq = # choices choices\n",
    "num_experiments = # choices choices\n",
    "lasso_vals = np.zeros(num_experiments*nmodel)\n",
    "lstsqs_vals = np.zeros(num_experiments*nmodel)\n",
    "for jj in range(num_experiments):\n",
    "    Amat = varsq*np.random.randn(nmeasure, nmodel)\n",
    "    bvec = varsq*np.random.randn(nmeasure)\n",
    "    res = Lasso(alpha=.1).fit(Amat, bvec.reshape(-1, 1)) # alpha > 0\n",
    "    alpha_lasso = res.coef_\n",
    "    alpha, error = least_squares_solve(Amat, bvec) # i.e. alpha = 0, or no LASSO\n",
    "    lasso_vals[jj*nmodel:(jj+1)*nmodel] = alpha_lasso\n",
    "    lstsqs_vals[jj*nmodel:(jj+1)*nmodel] = np.squeeze(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52003dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lasso_vals, density=True, bins=20, color='k');\n",
    "plt.hist(lstsqs_vals, density=True, bins=20, color='r', alpha=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087562f7",
   "metadata": {},
   "source": [
    "**Problem 2**: Now let's examine what happens when we vary $\\alpha$ in the prior problem and we might find a best choice for this critical parameter.  To do this, using the `xvals1, yvals1` data set with the model parameter $p=4$:   \n",
    "* For a fixed value of $\\alpha$, randomly remove $20\\%$ of the values in `yvals1`.  Use LASSO to fit to this data.\n",
    "* Use the remaining $20\\%$ to test quality of fit.  \n",
    "* Do this a meaningful number of times to get reasonable estimates for the mean and standard deviation of the error in your test of the fit.  \n",
    "\n",
    "See Sections 4.4 and 4.6 for further details and explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace9d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_experiment(alpha, xvals, yvals):\n",
    "    num_experiments = # choices choices\n",
    "    inds = np.arange(yvals.size)\n",
    "    errors = np.zeros(num_experiments)\n",
    "    train_size = round(.8*yvals.size)\n",
    "    for jj in range(num_experiments):\n",
    "        np.random.shuffle(inds)\n",
    "        shuffle_range = yvals[inds]\n",
    "        shuffle_domain = xvals[inds]\n",
    "        \n",
    "        bvec_train = shuffle_range[:train_size].reshape(-1,1)\n",
    "        bvec_test = shuffle_range[train_size:].reshape(-1,1)        \n",
    "        xp_train = xp_mat_maker(shuffle_domain[:train_size],4)\n",
    "        xp_test = xp_mat_maker(shuffle_domain[train_size:],4)\n",
    "        \n",
    "        res = Lasso(alpha, max_iter=4000).fit(xp_train, bvec_train) # alpha > 0\n",
    "        alpha_lasso = (res.coef_).reshape(-1,1)\n",
    "        errors[jj] = # I can't write all the code around here.          \n",
    "    mean_err = np.mean(errors)\n",
    "    stand_dev = np.std(errors)\n",
    "    return mean_err, stand_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940dbb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "apows = np.linspace() # choices, choices\n",
    "errors = np.zeros(apows.size)\n",
    "stds = np.zeros(apows.size)\n",
    "for jj, apow in enumerate(apows):\n",
    "    mean, std = validation_experiment(10**(apow), xvals1, yvals1) # alpha = 10^pow\n",
    "    errors[jj] = mean\n",
    "    stds[jj] = std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e886a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(apows, errors) # how would you plot the standard devations as error bars around the mean of the error?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2936d48-313e-4500-9f01-a69772d6f68d",
   "metadata": {},
   "source": [
    "**Problem 3**: We defined a convex function $f({\\bf x})$ over a convex set $E$ such that for $\\lambda\\in[0,1]$, \n",
    "$$\n",
    "f(\\lambda {\\bf x} + (1-\\lambda){\\bf y}) \\leq \\lambda f({\\bf x}) + (1-\\lambda) f({\\bf y}), ~ \\forall {\\bf x},{\\bf y}\\in E\n",
    "$$\n",
    "\n",
    "Show that if $f({\\bf x})$ is convex, then so is $f^{2}({\\bf x})$.  Note, you'll need to use the inequality\n",
    "$$\n",
    "2f({\\bf x})f({\\bf y}) \\leq f^{2}({\\bf x}) + f^{2}({\\bf y})\n",
    "$$\n",
    "to get the job done.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8d950f",
   "metadata": {},
   "source": [
    "**Problem 4**: (Graduate) Show that for a differentiable convex function, say $f({\\bf x})$ with gradient $\\nabla f$, that for all ${\\bf y}$ and ${\\bf x}$ in $E$ that\n",
    "$$\n",
    "f({\\bf y}) - f({\\bf x}) \\geq \\left<\\nabla_{{\\bf x}}f, {\\bf y}-{\\bf x}\\right>\n",
    "$$\n",
    "\n",
    "From this, show that a local minimum of a convex function is necessarily a global one.  To get the first result, swap the positions of ${\\bf x}$ and ${\\bf y}$ in the above definition of a convex function.  Then figure out how to get things in terms of difference quotients and then take a limit in $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e38ba0",
   "metadata": {},
   "source": [
    "**Problem 5**: For the function $f(x,y)$ where\n",
    "$$\n",
    "f(x,y) = ((x-1)^{2} + (y-1)^{2})((x-3)^{2} + (y-4)^{2})\n",
    "$$\n",
    "and $x, y \\in [-1 , 5]$, use gradient descent to find the minima relative to the initial conditions:\n",
    "$$\n",
    "(0, 2), ~\\mbox{and} ~(4, 0).\n",
    "$$\n",
    "\n",
    "If a descent step from ${\\bf x}_{n}=(x_{n}, y_{n})$ to ${\\bf x}_{n+1}=(x_{n+1}, y_{n+1})$ is given by\n",
    "$$\n",
    "{\\bf x}_{n+1} = {\\bf x}_{n} - \\lambda \\frac{\\nabla f}{\\left|\\left|\\nabla f\\right|\\right|_{2}},\n",
    "$$\n",
    "explain how you choose $\\lambda>0$ in order to make the method converge.  See Section 4.2 for further discussion and details.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8912a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,y):\n",
    "    return ((x - 1)**2. + (y - 1)**2.) * ( (x - 3)**2. + (y - 4)**2.)\n",
    "def gradf(xpt):\n",
    "    grad = np.zeros(2)\n",
    "    x = xpt[0]\n",
    "    y = xpt[1]\n",
    "    grad[0] = 2*(x-1)*( (x - 3)**2. + (y - 4)**2. ) + 2*(x-3)*((x - 1)**2. + (y - 1)**2.)\n",
    "    grad[1] = 2*(y-1)*( (x - 3)**2. + (y - 4)**2. ) + 2*(y-4)*((x - 1)**2. + (y - 1)**2.)\n",
    "    return grad\n",
    "\n",
    "def descent_stp(xpt, stpval):\n",
    "    gvec = gradf(xpt)\n",
    "    xp1 = xpt - stpval * gvec/np.linalg.norm(gvec)\n",
    "    return xp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dd08e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Npts = int(1e2)\n",
    "xvals = np.linspace(-1., 5., Npts)\n",
    "yvals = np.linspace(-1., 5., Npts)\n",
    "fvals = np.zeros((Npts, Npts))\n",
    "for jj in range(Npts):\n",
    "    for kk in range(Npts):\n",
    "        fvals[jj, kk] = f(xvals[jj], yvals[kk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b79c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just meant to get you started.  You need to think a bit more about how to make this work well.  \n",
    "\n",
    "xn = np.array([0., 2.])\n",
    "stpval = 1e-1\n",
    "    \n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "cplot = ax.contour(yvals, xvals, fvals, 80)\n",
    "\n",
    "for jj in range(100):    \n",
    "    xnp1 = descent_stp(xn, stpval)\n",
    "    dvals = xnp1 - xn\n",
    "    ax.arrow(xn[0], xn[1], dvals[0], dvals[1], width = .01)    \n",
    "    xn = copy(xnp1)\n",
    "    \n",
    "cbar = fig.colorbar(cplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2744c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
