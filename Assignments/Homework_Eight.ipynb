{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "36bb4bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fft import fft, ifft, fftshift\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from copy import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7bf0dc46-41dd-44ce-9b33-67063fc223b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_set_2.pkl', 'rb') as f:\n",
    "    data_set_2 = pickle.load(f)\n",
    "\n",
    "xvals1 = data_set_2[:, 0]\n",
    "yvals1 = data_set_2[:, 1]  + 2. + 3.*xvals1**2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale data\n",
    "xmax = np.max(xvals1)\n",
    "ymean = np.mean(yvals1)\n",
    "ystd = np.std(yvals1)\n",
    "xvalsrsc = xvals1/xmax\n",
    "yvalsrsc = (yvals1-ymean)/ystd\n",
    "plt.scatter(xvalsrsc, yvalsrsc, s=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106f4aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdata = np.abs(np.fft.fft(yvalsrsc))\n",
    "# something you could play with here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2b819075-29a7-4bec-9cee-fe1d5150fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xp_mat_maker(xvals, p, ovals):\n",
    "    xp = np.ones((xvals.size, p + 1 + 2*ovals.size))\n",
    "    for jj in range(p):\n",
    "        xp[:, jj+1] = xvals * xp[:, jj]\n",
    "    for jj in range(ovals.size):\n",
    "        xp[:, jj+p+1] = np.cos(ovals[jj]*xvals)\n",
    "        xp[:, jj+1+p+1] = np.sin(ovals[jj]*xvals)\n",
    "    return xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a3417f29-366c-4ed4-abea-0eb4a5fcb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_solve(mat, yvals):\n",
    "    u, s, vt = np.linalg.svd(mat, full_matrices=False)\n",
    "    alpha = (vt.T @ np.diag(1./s) @ u.T) @ yvals.reshape(-1, 1)\n",
    "    error = np.linalg.norm(mat @ alpha - yvals.reshape(-1, 1))**2./(2.*yvals.size)\n",
    "    return alpha, error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a79d9350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_fold(alpha, xvals, yvals, pval, ovals, kfolds):\n",
    "\n",
    "    kf = KFold(n_splits=kfolds)\n",
    "    w_avg = np.zeros(pval+1+2*ovals.size)\n",
    "    errors = np.zeros(kfolds)\n",
    "    cnt = 0\n",
    "\n",
    "    for train, test in kf.split(xvals):\n",
    "        xtrain, ytrain = xvals[train], yvals[train]\n",
    "        xtest, ytest = xvals[test], yvals[test]\n",
    "        xp_train = xp_mat_maker(xtrain, pval, ovals)        \n",
    "        res = Lasso(alpha, max_iter=4000, tol=1e-4).fit(xp_train, ytrain) # alpha > 0\n",
    "        w_lasso = res.coef_\n",
    "        w_avg += w_lasso\n",
    "        \n",
    "        xp_test = xp_mat_maker(xtest, pval, ovals)\n",
    "        errors[cnt] = np.linalg.norm(xp_test @ w_lasso.reshape(-1,1) - ytest)/np.sqrt(2.*ytest.size)\n",
    "        cnt += 1\n",
    "        \n",
    "    error = np.mean(errors)\n",
    "    var = np.std(errors)\n",
    "    return w_avg/kfolds, error, var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5779d8a3-bdf0-48be-8bfe-0bf6cf77ac1f",
   "metadata": {},
   "source": [
    "**Problem 1**: To get a feel for how LASSO regression works, we will study the problem of solving \n",
    "$$\n",
    "A{\\bf x} = {\\bf b}, ~ A \\in \\mathbb{R}^{n\\times m}, {\\bf b} \\in \\mathbb{R}^{n}\n",
    "$$\n",
    "via least squares with and without LASSO.  Thus we study minimizing the objective function $f({\\bf x})$ where \n",
    "$$\n",
    "f({\\bf x}) = \\frac{1}{2n}\\left|\\left|{\\bf b} - A{\\bf x}\\right|\\right|^{2}_{2} + \\alpha \\left|\\left|{\\bf x}\\right|\\right|_{1}, ~ \\alpha \\geq 0.\n",
    "$$\n",
    "\n",
    "We choose $A$ and ${\\bf b}$ at random from Gaussian distributions of zero mean and variance $\\sigma$.  By varying $n$, $m$, and $\\sigma$, generate distributions of the values of ${\\bf x}$ for $\\alpha=10^{-4}$, $10^{-3}$, $10^{-2}$.  Characterize the impact of LASSO on the spread in the values of ${\\bf x}$.  Also, generate affiliated distributions of the error in your fits.  Describe any differences you see between LASSO fits and fits done with classic least-squares.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0fa74660-4712-4722-9828-bddaddabb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmodel = # choices\n",
    "nmeasure = # more choices\n",
    "varsq = # another choice yet still\n",
    "num_experiments = # more of these, the better \n",
    "aval = 1e-2\n",
    "\n",
    "lasso_vals = np.zeros(num_experiments*nmodel)\n",
    "lstsqs_vals = np.zeros(num_experiments*nmodel)\n",
    "for jj in range(num_experiments):\n",
    "    Amat = varsq*np.random.randn(nmeasure, nmodel)\n",
    "    bvec = varsq*np.random.randn(nmeasure)\n",
    "    res = Lasso(alpha=aval).fit(Amat, bvec.reshape(-1, 1)) # alpha > 0\n",
    "    alpha_lasso = res.coef_\n",
    "    alpha, error = least_squares_solve(Amat, bvec) # i.e. alpha = 0, or no LASSO\n",
    "    lasso_vals[jj*nmodel:(jj+1)*nmodel] = alpha_lasso\n",
    "    lstsqs_vals[jj*nmodel:(jj+1)*nmodel] = np.squeeze(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52003dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lasso_vals, density=True, bins=20, color='k');\n",
    "plt.hist(lstsqs_vals, density=True, bins=20, color='r', alpha=.5);\n",
    "\n",
    "# there is only one plot here just for referencing the commands.  you should probably present several."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087562f7",
   "metadata": {},
   "source": [
    "**Problem 2**: Now, using the `xvals1, yvals1` data set, suppose we build a general model of the form \n",
    "$$\n",
    "y(x) = \\sum_{l=0}^{p}\\alpha_{l}x^{l} + \\sum_{j=1}^{m}\\left(\\beta_{j}\\cos(\\omega_{j}x)+\\gamma_{j}\\sin(\\omega_{j}x)\\right)\n",
    "$$\n",
    "\n",
    "* Choose some $p$, $m$, and corresponding range of frequences $\\omega_{j}$.  Note, the point of LASSO is to be able to build general models and let the optimization tell you what to keep, so this is supposed to be relatively broad.  \n",
    "\n",
    "* Using the 10-fold split as in lecture, repeat the analysis in the lecture notes but now for this more complex model.  Comment on the quality of LASSO in finding good fits while also making meaningful parameter selections.  \n",
    "\n",
    "* What do you think the polynomial part of the data is?  What about the trigonometric part?  What does your best fit look like in comparison to the data?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19acd56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numlams = 30\n",
    "lampows = np.linspace(-5, 0., numlams) # this could be changed depending on you make other decisions\n",
    "errors = np.zeros(lampows.size)\n",
    "stds = np.zeros(lampows.size)\n",
    "pval = # choices\n",
    "ovals = # even more choices, and keep in mind, you might want to make one choice and then adjust this to see if you can't improve the results\n",
    "kfolds = 10\n",
    "avalsvslam = np.zeros((pval+1+2*ovals.size, numlams))\n",
    "\n",
    "for jj, lampow in enumerate(lampows):\n",
    "    lam = 10**(lampow)    \n",
    "    w_avg, mean, std = cross_fold(lam, xvalsrsc, yvalsrsc, pval, ovals, kfolds)\n",
    "    avalsvslam[:, jj] = w_avg\n",
    "    errors[jj] = mean\n",
    "    stds[jj] = std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246582b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lampows, errors) # how would you plot the standard devations as error bars around the mean of the error?\n",
    "for jj in range(stds.size):\n",
    "    ejp = errors[jj] + stds[jj]\n",
    "    ejm = errors[jj] - stds[jj]\n",
    "    plt.plot([lampows[jj], lampows[jj]], [ejm, ejp], ls='--', c='r')\n",
    "plt.xlabel(r\"$\\log_{10}(\\lambda)$\");\n",
    "plt.ylabel(r\"Mean Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ff306",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfig = plt.pcolormesh(lampows, np.arange(pval+1+2*ovals.size), np.ma.log10(avalsvslam))\n",
    "plt.xlabel(r\"$\\log_{10}(\\lambda)$\")\n",
    "plt.ylabel(r\"$\\alpha_{j}$\")\n",
    "plt.colorbar(cfig);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b124ac",
   "metadata": {},
   "source": [
    "**Problem 3** (Graduate): Since there was so much interest in this problem, I'll have y'all work it out.  So, suppose that, for ${\\bf y}, {\\bf z} \\in \\mathbb{R}^{n}$ we want to solve \n",
    "\n",
    "$$\n",
    "{\\bf z}_{\\ast} = \\text{arg min}_{{\\bf z}} ~ \\left|\\left|{\\bf y} - A{\\bf z}\\right|\\right|_{2}^{2} + \\lambda \\left|\\left|{\\bf z}\\right|\\right|_{1} + \\gamma \\left|\\left|{\\bf z}\\right|\\right|_{2}^{2}\n",
    "$$\n",
    "\n",
    "where $\\lambda, ~ \\gamma > 0$ and for real-valued, square $A$ such that $A^{T}A = AA^{T} = I$.  Letting ${\\bf u} = A^{T}{\\bf y}$, show that \n",
    "\n",
    "$$\n",
    "z_{\\ast,l} = \\left\\{\n",
    "\\begin{array}{rl}\n",
    "\\frac{1}{1+\\gamma}\\left(u_{l} - \\frac{\\lambda}{2}\\right) & u_{l} > \\frac{\\lambda}{2} \\\\\n",
    "0 & |u_{l}| \\leq \\frac{\\lambda}{2} \\\\ \n",
    "\\frac{1}{1+\\gamma}\\left(u_{l} + \\frac{\\lambda}{2}\\right) & u_{l} < -\\frac{\\lambda}{2} \n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Show that this multi-part formula can be written in the freakishly compact form\n",
    "\n",
    "$$\n",
    "z_{\\ast,l} = \\frac{u_{l}}{1+\\gamma}\\text{max}\\left\\{0, 1 - \\frac{\\lambda}{2|u_{l}|}\\right\\}\n",
    "$$\n",
    "\n",
    "Note, to do all of this, you'll need the sub-differential of $|x|$, $\\partial |x|$, where \n",
    "\n",
    "$$\n",
    "\\partial |x| = \\left\\{\n",
    "\\begin{array}{rl}\n",
    "1 & x > 0\\\\\n",
    "r\\in(-1,1)& x=0, ~ \\\\\n",
    "-1 & x < 0\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8d950f",
   "metadata": {},
   "source": [
    "**Problem 4**: (Graduate) Show that for a differentiable convex function, say $f({\\bf x})$ with gradient $\\nabla f$, that for all ${\\bf y}$ and ${\\bf x}$ in $E$ that\n",
    "$$\n",
    "f({\\bf y}) - f({\\bf x}) \\geq \\left<\\nabla_{{\\bf x}}f, {\\bf y}-{\\bf x}\\right>\n",
    "$$\n",
    "\n",
    "From this, show that a local minimum of a convex function is necessarily a global one.  To get the first result, swap the positions of ${\\bf x}$ and ${\\bf y}$ in the above definition of a convex function.  Then figure out how to get things in terms of difference quotients and then take a limit in $\\lambda$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math_596_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
